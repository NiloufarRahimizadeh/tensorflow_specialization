{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c153ef88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text (comment)</th>\n",
       "      <th>Spam or ham</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ÛŒÙ‡ Ù…Ø´Øª Ù„Ø§Øª Ùˆ Ù„ÙˆØª Ø¬Ù…Ø¹ Ú©Ø±Ø¯ÛŒØ¯ ØªÙˆ Ø§ÛŒÙ† Ø§ØªØ§Ù‚ ÙØ±Ø§Ø± Ùˆ ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ø³Ù†Ø§Ø±ÛŒÙˆ Ø§ØµÙ„Ø§ Ø®ÙˆØ¨ Ù†Ø¨ÙˆØ¯ Ùˆ Ø¨Ø±Ø§ÛŒ Ù…Ø§ Ù†ØµÙÙ‡ ØªÙ…ÙˆÙ… Ø´Ø¯ - ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ø±ÙØªØ§Ø± Ù¾Ø±Ø³Ù†Ù„ Ù…Ù†Ø§Ø³Ø¨ Ù†Ø¨ÙˆØ¯\\n  Ø¹Ø¯Ù… Ø§Ú¯Ø§Ù‡ÛŒ Ø±Ø³Ø§Ù†ÛŒ Ø¯Ù‚ÛŒÙ‚...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ğŸ˜¡ğŸ˜¡ğŸ˜¡Ù‡Ø´Ø¯Ø§Ø± Ø§ÛŒÙ† ÛŒÚ© Ú©Ù„Ø§Ù‡ Ø¨Ø±Ø¯Ø§Ø±ÛŒ Ø¹Ù„Ù†ÛŒ Ø§Ø³ØªğŸ˜¡ğŸ˜¡ğŸ˜¡\\nØ®ÙˆÙ†Ù‡ ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ø§ØªØ§Ù‚ ÙØ±Ø§Ø± Ø®ÙˆØ¨ÛŒ Ø¨ÙˆØ¯ Ø§Ù…Ø§ Ù†Ù‡ Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú©Ø§Ù…Ù†Øª Ù‡Ø§ Ù‚...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Text (comment) Spam or ham \n",
       "0  ÛŒÙ‡ Ù…Ø´Øª Ù„Ø§Øª Ùˆ Ù„ÙˆØª Ø¬Ù…Ø¹ Ú©Ø±Ø¯ÛŒØ¯ ØªÙˆ Ø§ÛŒÙ† Ø§ØªØ§Ù‚ ÙØ±Ø§Ø± Ùˆ ...         Spam\n",
       "1  Ø³Ù†Ø§Ø±ÛŒÙˆ Ø§ØµÙ„Ø§ Ø®ÙˆØ¨ Ù†Ø¨ÙˆØ¯ Ùˆ Ø¨Ø±Ø§ÛŒ Ù…Ø§ Ù†ØµÙÙ‡ ØªÙ…ÙˆÙ… Ø´Ø¯ - ...         Spam\n",
       "2  Ø±ÙØªØ§Ø± Ù¾Ø±Ø³Ù†Ù„ Ù…Ù†Ø§Ø³Ø¨ Ù†Ø¨ÙˆØ¯\\n  Ø¹Ø¯Ù… Ø§Ú¯Ø§Ù‡ÛŒ Ø±Ø³Ø§Ù†ÛŒ Ø¯Ù‚ÛŒÙ‚...         Spam\n",
       "3  ğŸ˜¡ğŸ˜¡ğŸ˜¡Ù‡Ø´Ø¯Ø§Ø± Ø§ÛŒÙ† ÛŒÚ© Ú©Ù„Ø§Ù‡ Ø¨Ø±Ø¯Ø§Ø±ÛŒ Ø¹Ù„Ù†ÛŒ Ø§Ø³ØªğŸ˜¡ğŸ˜¡ğŸ˜¡\\nØ®ÙˆÙ†Ù‡ ...         Spam\n",
       "4  Ø§ØªØ§Ù‚ ÙØ±Ø§Ø± Ø®ÙˆØ¨ÛŒ Ø¨ÙˆØ¯ Ø§Ù…Ø§ Ù†Ù‡ Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú©Ø§Ù…Ù†Øª Ù‡Ø§ Ù‚...         Spam"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "data_path = '/Users/niloufar/Desktop/DeepLearning/tf_specialization/comment/'\n",
    "data1 = 'spam_or_not1.xlsx'\n",
    "data2 = 'spam_or_not2.xlsx'\n",
    "data3 = 'spam_or_not3.xlsx'\n",
    "df1 = pd.read_excel(data_path + data1)\n",
    "df2 = pd.read_excel(data_path + data2)\n",
    "df3 = pd.read_excel(data_path + data3)\n",
    "df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "df = df.drop(['ID', df.columns[3]], axis=1)\n",
    "# df.iloc[995:1005, :]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8603b2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(sentence):\n",
    "    stopwords = ['Ø±Ø§', 'Ø¨Ù‡', 'Ùˆ', 'Ø§Ø²', 'Ú©Ù‡']\n",
    "    words = sentence.split()\n",
    "    results_words = [word for word in words if word not in stopwords]\n",
    "    sentence = ' '.join(results_words)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95990b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ù¾Ø¯Ø± Ø³Ú¯ Ø¯Ø±Ø®Øª Ø¨Ø³Øª.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(\"Ù¾Ø¯Ø± Ø³Ú¯ Ø±Ø§ Ø¨Ù‡ Ø¯Ø±Ø®Øª Ø¨Ø³Øª.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b77f11",
   "metadata": {},
   "source": [
    "### Reading the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd1878b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_from_file(file):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for i, j in zip(df[df.columns[0]], df[df.columns[1]]):\n",
    "        sentences.append(remove_stopwords(i))\n",
    "        labels.append(j)\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7457fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, labels = parse_data_from_file(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a794f011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6305 sentences in the dataset.\n",
      "\n",
      "First sentence has 104 words (after removing stopwords).\n",
      "\n",
      "There are 6305 labels in the dataset.\n",
      "\n",
      "The first 5 labels are ['Spam', 'Spam', 'Spam', 'Spam', 'Spam']\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(sentences)} sentences in the dataset.\\n\")\n",
    "print(f\"First sentence has {len(sentences[0].split())} words (after removing stopwords).\\n\")\n",
    "print(f\"There are {len(labels)} labels in the dataset.\\n\")\n",
    "print(f\"The first 5 labels are {labels[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4a3b18",
   "metadata": {},
   "source": [
    "### Using the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e786310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tokenizer(sentences):\n",
    "    tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34391a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary contains 15232 words\n",
      "\n",
      "<OOV> token included in vocabulary\n"
     ]
    }
   ],
   "source": [
    "tokenizer = fit_tokenizer(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(f\"Vocabulary contains {len(word_index)} words\\n\")\n",
    "print(\"<OOV> token included in vocabulary\" if \"<OOV>\" in word_index else \"<OOV> token NOT included in vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a26c516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padded_sequences(tokenizer, sentences):\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    padded_sequences = pad_sequences(sequences, padding='post')\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac35a805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First padded sequence looks like this: \n",
      "\n",
      "[  28 3383 6060 6061  573  848   36    9    8   20  110 1568  189  272\n",
      "    3  688   10  344    3  865  131   16 1274 4251 6062  776   56  689\n",
      "   16 4252 1802 1569 6063  147 1211 4253 4254  221 6064  508  122  443\n",
      " 6065  339 6066  795   32 6067 6068    7 1484   78 1802 1569  573  273\n",
      "   38  429 6069    9 1275 1007  113 3384  122 1803 1804  307 6070   28\n",
      "   18   92   41   38  308 6071  322 4255 6072  182    9    8   20   12\n",
      "  204 6073  204 1396  522 1043 1070 6074  509  452 6075   39   10 6076\n",
      " 2451 4256    9    8   20   12 2182    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0]\n",
      "\n",
      "Numpy array of all sequences has shape: (6305, 819)\n",
      "\n",
      "This means there are 6305 sequences in total and each one has a size of 819\n"
     ]
    }
   ],
   "source": [
    "padded_sequences = get_padded_sequences(tokenizer, sentences)\n",
    "print(f\"First padded sequence looks like this: \\n\\n{padded_sequences[0]}\\n\")\n",
    "print(f\"Numpy array of all sequences has shape: {padded_sequences.shape}\\n\")\n",
    "print(f\"This means there are {padded_sequences.shape[0]} sequences in total and each one has a size of {padded_sequences.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41ec0b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_labels(labels):\n",
    "    label_tokenizer = Tokenizer()\n",
    "    label_tokenizer.fit_on_texts(labels)\n",
    "    label_word_index = label_tokenizer.word_index\n",
    "    label_sequences = label_tokenizer.texts_to_sequences(labels)\n",
    "    return label_sequences, label_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56390c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary of labels looks like this {'ham': 1, 'spam': 2}\n",
      "\n",
      "First ten sequences [[2], [2], [2], [2], [2], [2], [2], [2], [2], [2]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_sequences, label_word_index = tokenize_labels(labels)\n",
    "print(f\"Vocabulary of labels looks like this {label_word_index}\\n\")\n",
    "print(f\"First ten sequences {label_sequences[:10]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7383d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269197c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c3728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715405de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94b7054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf38",
   "language": "python",
   "name": "tf38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
