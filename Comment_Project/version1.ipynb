{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ad7f01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e168f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text (comment)</th>\n",
       "      <th>Spam or ham</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>یه مشت لات و لوت جمع کردید تو این اتاق فرار و ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>سناریو اصلا خوب نبود و برای ما نصفه تموم شد - ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>رفتار پرسنل مناسب نبود\\n  عدم اگاهی رسانی دقیق...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>😡😡😡هشدار این یک کلاه برداری علنی است😡😡😡\\nخونه ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>اتاق فرار خوبی بود اما نه به اندازه کامنت ها ق...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>سلام\\nمجوز برای اتاق فرار نداشتند\\nبرخورد خوب ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>داستان بازی سر و ته مشخصی نداشت،بیشتر فضا سازی...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>در خصوص بازی های اتاق فرار با احترام به کسانی ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>نه مجوز\\nنه کوچه مطمئن \\nنه محله درست\\nنه جای ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>افتضاح خواهشن به كامنت ها توجه نكنيد\\nنميدونم ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>سلام اتاق واقعا فضای بزرگی داشت و طبقات زیاد ا...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>بی ناموساااا</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>به نظر من واقعا از هیچ نظر خوب نبود، چون قبل ا...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>اصلا جالب نبود قفل ها همه خراب  ما برای پیدا ک...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>سلام ما دیشب ساعت ۱۲ سانس ویژه بازیو رزرو کرده...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>افتضاح\\nدکور و فضا سازی خدایی خوب بود \\nولی اک...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ما اولش انصراف دادیم ولی رفتار پرسنل بسیار بد ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>بسیار ضعیف بود.دکور ضعیف،سناریو ضعیف،بازی اکتو...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Text (comment) Spam or ham \n",
       "0   یه مشت لات و لوت جمع کردید تو این اتاق فرار و ...         Spam\n",
       "1   سناریو اصلا خوب نبود و برای ما نصفه تموم شد - ...         Spam\n",
       "2   رفتار پرسنل مناسب نبود\\n  عدم اگاهی رسانی دقیق...         Spam\n",
       "3   😡😡😡هشدار این یک کلاه برداری علنی است😡😡😡\\nخونه ...         Spam\n",
       "4   اتاق فرار خوبی بود اما نه به اندازه کامنت ها ق...         Spam\n",
       "5   سلام\\nمجوز برای اتاق فرار نداشتند\\nبرخورد خوب ...         Spam\n",
       "6   داستان بازی سر و ته مشخصی نداشت،بیشتر فضا سازی...         Spam\n",
       "7   در خصوص بازی های اتاق فرار با احترام به کسانی ...         Spam\n",
       "8   نه مجوز\\nنه کوچه مطمئن \\nنه محله درست\\nنه جای ...         Spam\n",
       "9   افتضاح خواهشن به كامنت ها توجه نكنيد\\nنميدونم ...         Spam\n",
       "10  سلام اتاق واقعا فضای بزرگی داشت و طبقات زیاد ا...         Spam\n",
       "11                                       بی ناموساااا         Spam\n",
       "12  به نظر من واقعا از هیچ نظر خوب نبود، چون قبل ا...         Spam\n",
       "13  اصلا جالب نبود قفل ها همه خراب  ما برای پیدا ک...         Spam\n",
       "14  سلام ما دیشب ساعت ۱۲ سانس ویژه بازیو رزرو کرده...         Spam\n",
       "15  افتضاح\\nدکور و فضا سازی خدایی خوب بود \\nولی اک...         Spam\n",
       "16  ما اولش انصراف دادیم ولی رفتار پرسنل بسیار بد ...         Spam\n",
       "17  بسیار ضعیف بود.دکور ضعیف،سناریو ضعیف،بازی اکتو...         Spam"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '/Users/niloufar/Desktop/DeepLearning/tf_specialization/comment/'\n",
    "data1 = 'spam_or_not1.xlsx'\n",
    "data2 = 'spam_or_not2.xlsx'\n",
    "data3 = 'spam_or_not3.xlsx'\n",
    "\n",
    "df1 = pd.read_excel(data_path + data1)\n",
    "df2 = pd.read_excel(data_path + data2)\n",
    "df3 = pd.read_excel(data_path + data3)\n",
    "\n",
    "df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "df = df.drop(['ID', df.columns[3]], axis=1)\n",
    "df.head(18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62379bb",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b045576",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels'] = df.apply(lambda x: x[1].lower(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e4d00b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=df.columns[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1819956f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "ham     5520\n",
       "spam     715\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.columns[1]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5c9526e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11040"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grouped_by = df.groupby('labels')\n",
    "df_balanced = df_grouped_by.apply(lambda x: x.sample(df_grouped_by.size().max(), replace=True).reset_index(drop=True))\n",
    "len(df_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d44b2669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "ham     5520\n",
       "spam    5520\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced[df_balanced.columns[1]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d4b8119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Text (comment)'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced.columns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5394b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = {'class':['c1','c2','c1','c1','c2','c1','c1','c2','c3','c3'],\n",
    "#      'val': [1,2,1,1,2,1,1,2,3,3]\n",
    "#     }\n",
    "\n",
    "# df = pd.DataFrame(d)\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "375546d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = df.groupby('class')\n",
    "# b = g.apply(lambda x: x.sample(g.size().max(), replace=True).reset_index(drop=True))\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7783ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(sentence):\n",
    "    stopwords = ['که', 'در', 'از', 'به', 'و', 'را', 'این', 'آن', 'بعد', 'همه', 'دوباره', 'یک', 'یه', 'من', 'تو', 'او', 'ما', 'شما', 'قبل', 'آنها', 'زیرا', 'زیر', 'اما', 'بین', 'دو', 'با', 'اونجا',\n",
    "                'برای', 'حتما','حالی', 'چرا', 'چی', 'ازطریق', 'رو', ',', 'واقعا','ها', 'تو', 'اون', 'ترین', 'توی', 'چه', 'مارو', 'سر', 'اونجا', 'خود', 'هارو', 'آقا', 'همتون', 'هام', 'دوتا', 'دوباره',\n",
    "                'اگه', 'ولی', 'روش', 'اینو', 'هنوز', 'ده', 'سه', 'چهار', 'پنج', 'شش', 'هفت', 'هشت', 'نه', 'ده', 'نا','ایی']\n",
    "    words = sentence.split()\n",
    "    results_words = [word for word in words if word not in stopwords]\n",
    "    sentence = ' '.join(results_words)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1b0b89f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'پدر سگ درخت بست.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(\"پدر سگ را به درخت بست.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6965fc20",
   "metadata": {},
   "source": [
    "### Reading the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b70cf449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_from_file(df):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for i,j in zip(df[df.columns[0]], df[df.columns[1]]):\n",
    "        sentences.append(remove_stopwords(i))\n",
    "        labels.append(j)\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1dc41920",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, labels = parse_data_from_file(df_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e082d1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11040 sentences in the dataset.\n",
      "\n",
      "First sentence has 30 words (after removing stopwords).\n",
      "\n",
      "There are 11040 labels in the dataset.\n",
      "\n",
      "The first 5 labels are ['ham', 'ham', 'ham', 'ham', 'ham']\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(sentences)} sentences in the dataset.\\n\")\n",
    "print(f\"First sentence has {len(sentences[0].split())} words (after removing stopwords).\\n\")\n",
    "print(f\"There are {len(labels)} labels in the dataset.\\n\")\n",
    "print(f\"The first 5 labels are {labels[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f0d073d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 10000 #10894\n",
    "EMBEDDING_DIM = 64\n",
    "MAXLEN = 120 #681\n",
    "PADDING = 'post'\n",
    "OOV_TOKEN = '<OOV>'\n",
    "TRAINING_SPLIT = .8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3642c075",
   "metadata": {},
   "source": [
    "### Training - Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8e9019e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(sentences, labels, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d2ae9d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_val_split(sentences, labels, training_split):\n",
    "#     train_size = int(len(sentences)*training_split)\n",
    "#     train_sentences = sentences[:train_size]\n",
    "#     train_labels = labels[:train_size]\n",
    "    \n",
    "#     validation_sentences = sentences[train_size:]\n",
    "#     validation_labels = labels[train_size:]\n",
    "#     return train_sentences, validation_sentences, train_labels, validation_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c93e29bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8832 sentences for training.\n",
      "\n",
      "There are 8832 labels for training.\n",
      "\n",
      "There are 2208 sentences for validation.\n",
      "\n",
      "There are 2208 labels for validation.\n"
     ]
    }
   ],
   "source": [
    "# train_sentences, val_sentences, train_labels, val_labels = train_val_split(sentences, labels, TRAINING_SPLIT)\n",
    "\n",
    "print(f\"There are {len(train_sentences)} sentences for training.\\n\")\n",
    "print(f\"There are {len(train_labels)} labels for training.\\n\")\n",
    "print(f\"There are {len(val_sentences)} sentences for validation.\\n\")\n",
    "print(f\"There are {len(val_labels)} labels for validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fcd8b5",
   "metadata": {},
   "source": [
    "### Using the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2871dcd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4423, 1097)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.count('spam'), val_labels.count('spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "73f46b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tokenizer(sentences, num_words, oov_token):\n",
    "    tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4caa9d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary contains 10786 words\n",
      "\n",
      "<OOV> token included in vocabulary\n"
     ]
    }
   ],
   "source": [
    "tokenizer = fit_tokenizer(train_sentences, NUM_WORDS, OOV_TOKEN)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(f\"Vocabulary contains {len(tokenizer.word_index)} words\\n\")\n",
    "print(\"<OOV> token included in vocabulary\" if \"<OOV>\" in tokenizer.word_index else \"<OOV> token NOT included in vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c46690b",
   "metadata": {},
   "source": [
    "### Save tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ee5dfffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open('tokenizer.pickle', 'wb') as handle:\n",
    "#     pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ec185d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_and_pad(tokenizer, sentences, padding, maxlen):\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    padded_sequences = pad_sequences(sequences, padding=padding, maxlen=maxlen)\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3ff32ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded training sequences have shape: (8832, 120)\n",
      "\n",
      "Padded validation sequences have shape: (2208, 120)\n"
     ]
    }
   ],
   "source": [
    "train_padded_seq = seq_and_pad(tokenizer,train_sentences, PADDING, MAXLEN)\n",
    "val_padded_seq = seq_and_pad(tokenizer, val_sentences, PADDING, MAXLEN)\n",
    "\n",
    "print(f\"Padded training sequences have shape: {train_padded_seq.shape}\\n\")\n",
    "print(f\"Padded validation sequences have shape: {val_padded_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f73a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_labels(all_labels, split_labels):\n",
    "#     label_tokenizer = Tokenizer()\n",
    "#     label_tokenizer.fit_on_texts(all_labels)\n",
    "#     label_seq = label_tokenizer.texts_to_sequences(split_labels)\n",
    "#     label_seq_np = np.array(label_seq) - 1\n",
    "    \n",
    "#     return label_seq_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90ee8cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('spam', 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_seq = np.unique(train_labels, return_inverse=True)[1] # spam==1\n",
    "train_labels[0], train_label_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba146e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('spam', 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_label_seq =np.unique(val_labels, return_inverse=True)[1]  # spam==1\n",
    "val_labels[0], val_label_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c98c36f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 labels of the training set should look like this:\n",
      "[1 1 0 1 1]\n",
      "\n",
      "First 5 labels of the validation set should look like this:\n",
      "[1 0 1 0 1]\n",
      "\n",
      "Tokenized labels of the training set have shape: (8832,)\n",
      "\n",
      "Tokenized labels of the validation set have shape: (2208,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train_label_seq = tokenize_labels(labels, train_labels)\n",
    "# val_label_seq = tokenize_labels(labels, val_labels)\n",
    "\n",
    "print(f\"First 5 labels of the training set should look like this:\\n{train_label_seq[:5]}\\n\")\n",
    "print(f\"First 5 labels of the validation set should look like this:\\n{val_label_seq[:5]}\\n\")\n",
    "print(f\"Tokenized labels of the training set have shape: {train_label_seq.shape}\\n\")\n",
    "print(f\"Tokenized labels of the validation set have shape: {val_label_seq.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e263abf1",
   "metadata": {},
   "source": [
    "### Selecting the model for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e747cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_words, embedding_dim, maxlen):\n",
    "    \n",
    "    tf.random.set_seed(123)\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(num_words, embedding_dim, input_length=MAXLEN),\n",
    "#         tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(6, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb85724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(NUM_WORDS, EMBEDDING_DIM, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd63c804",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_padded_seq, train_label_seq, epochs=30, validation_data=(val_padded_seq, val_label_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82480cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(val_padded_seq[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5152ec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_label_seq[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfba567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_graphs(history, metric):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history[f'val_{metric}'])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([metric, f'val_{metric}'])\n",
    "    plt.show()\n",
    "    \n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f94f2e4",
   "metadata": {},
   "source": [
    "### Save the entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5458e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p ../../saved_model\n",
    "model.save('../../saved_model/my_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ecabfacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 11:13:10.948468: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.32601893]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model('../../saved_model/my_model')\n",
    "new_model.predict(train_padded_seq[1:2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf38",
   "language": "python",
   "name": "tf38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
