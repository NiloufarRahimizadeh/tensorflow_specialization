{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ad7f01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e168f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text (comment)</th>\n",
       "      <th>Spam or ham</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ÛŒÙ‡ Ù…Ø´Øª Ù„Ø§Øª Ùˆ Ù„ÙˆØª Ø¬Ù…Ø¹ Ú©Ø±Ø¯ÛŒØ¯ ØªÙˆ Ø§ÛŒÙ† Ø§ØªØ§Ù‚ ÙØ±Ø§Ø± Ùˆ ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ø³Ù†Ø§Ø±ÛŒÙˆ Ø§ØµÙ„Ø§ Ø®ÙˆØ¨ Ù†Ø¨ÙˆØ¯ Ùˆ Ø¨Ø±Ø§ÛŒ Ù…Ø§ Ù†ØµÙÙ‡ ØªÙ…ÙˆÙ… Ø´Ø¯ - ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ø±ÙØªØ§Ø± Ù¾Ø±Ø³Ù†Ù„ Ù…Ù†Ø§Ø³Ø¨ Ù†Ø¨ÙˆØ¯\\n  Ø¹Ø¯Ù… Ø§Ú¯Ø§Ù‡ÛŒ Ø±Ø³Ø§Ù†ÛŒ Ø¯Ù‚ÛŒÙ‚...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ğŸ˜¡ğŸ˜¡ğŸ˜¡Ù‡Ø´Ø¯Ø§Ø± Ø§ÛŒÙ† ÛŒÚ© Ú©Ù„Ø§Ù‡ Ø¨Ø±Ø¯Ø§Ø±ÛŒ Ø¹Ù„Ù†ÛŒ Ø§Ø³ØªğŸ˜¡ğŸ˜¡ğŸ˜¡\\nØ®ÙˆÙ†Ù‡ ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ø§ØªØ§Ù‚ ÙØ±Ø§Ø± Ø®ÙˆØ¨ÛŒ Ø¨ÙˆØ¯ Ø§Ù…Ø§ Ù†Ù‡ Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú©Ø§Ù…Ù†Øª Ù‡Ø§ Ù‚...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ø³Ù„Ø§Ù…\\nÙ…Ø¬ÙˆØ² Ø¨Ø±Ø§ÛŒ Ø§ØªØ§Ù‚ ÙØ±Ø§Ø± Ù†Ø¯Ø§Ø´ØªÙ†Ø¯\\nØ¨Ø±Ø®ÙˆØ±Ø¯ Ø®ÙˆØ¨ ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ø¯Ø§Ø³ØªØ§Ù† Ø¨Ø§Ø²ÛŒ Ø³Ø± Ùˆ ØªÙ‡ Ù…Ø´Ø®ØµÛŒ Ù†Ø¯Ø§Ø´ØªØŒØ¨ÛŒØ´ØªØ± ÙØ¶Ø§ Ø³Ø§Ø²ÛŒ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ø¯Ø± Ø®ØµÙˆØµ Ø¨Ø§Ø²ÛŒ Ù‡Ø§ÛŒ Ø§ØªØ§Ù‚ ÙØ±Ø§Ø± Ø¨Ø§ Ø§Ø­ØªØ±Ø§Ù… Ø¨Ù‡ Ú©Ø³Ø§Ù†ÛŒ ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ù†Ù‡ Ù…Ø¬ÙˆØ²\\nÙ†Ù‡ Ú©ÙˆÚ†Ù‡ Ù…Ø·Ù…Ø¦Ù† \\nÙ†Ù‡ Ù…Ø­Ù„Ù‡ Ø¯Ø±Ø³Øª\\nÙ†Ù‡ Ø¬Ø§ÛŒ ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ø§ÙØªØ¶Ø§Ø­ Ø®ÙˆØ§Ù‡Ø´Ù† Ø¨Ù‡ ÙƒØ§Ù…Ù†Øª Ù‡Ø§ ØªÙˆØ¬Ù‡ Ù†ÙƒÙ†ÙŠØ¯\\nÙ†Ù…ÙŠØ¯ÙˆÙ†Ù… ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Ø³Ù„Ø§Ù… Ø§ØªØ§Ù‚ ÙˆØ§Ù‚Ø¹Ø§ ÙØ¶Ø§ÛŒ Ø¨Ø²Ø±Ú¯ÛŒ Ø¯Ø§Ø´Øª Ùˆ Ø·Ø¨Ù‚Ø§Øª Ø²ÛŒØ§Ø¯ Ø§...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ø¨ÛŒ Ù†Ø§Ù…ÙˆØ³Ø§Ø§Ø§Ø§</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Ø¨Ù‡ Ù†Ø¸Ø± Ù…Ù† ÙˆØ§Ù‚Ø¹Ø§ Ø§Ø² Ù‡ÛŒÚ† Ù†Ø¸Ø± Ø®ÙˆØ¨ Ù†Ø¨ÙˆØ¯ØŒ Ú†ÙˆÙ† Ù‚Ø¨Ù„ Ø§...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Ø§ØµÙ„Ø§ Ø¬Ø§Ù„Ø¨ Ù†Ø¨ÙˆØ¯ Ù‚ÙÙ„ Ù‡Ø§ Ù‡Ù…Ù‡ Ø®Ø±Ø§Ø¨  Ù…Ø§ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ú©...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ø³Ù„Ø§Ù… Ù…Ø§ Ø¯ÛŒØ´Ø¨ Ø³Ø§Ø¹Øª Û±Û² Ø³Ø§Ù†Ø³ ÙˆÛŒÚ˜Ù‡ Ø¨Ø§Ø²ÛŒÙˆ Ø±Ø²Ø±Ùˆ Ú©Ø±Ø¯Ù‡...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Ø§ÙØªØ¶Ø§Ø­\\nØ¯Ú©ÙˆØ± Ùˆ ÙØ¶Ø§ Ø³Ø§Ø²ÛŒ Ø®Ø¯Ø§ÛŒÛŒ Ø®ÙˆØ¨ Ø¨ÙˆØ¯ \\nÙˆÙ„ÛŒ Ø§Ú©...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Ù…Ø§ Ø§ÙˆÙ„Ø´ Ø§Ù†ØµØ±Ø§Ù Ø¯Ø§Ø¯ÛŒÙ… ÙˆÙ„ÛŒ Ø±ÙØªØ§Ø± Ù¾Ø±Ø³Ù†Ù„ Ø¨Ø³ÛŒØ§Ø± Ø¨Ø¯ ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Ø¨Ø³ÛŒØ§Ø± Ø¶Ø¹ÛŒÙ Ø¨ÙˆØ¯.Ø¯Ú©ÙˆØ± Ø¶Ø¹ÛŒÙØŒØ³Ù†Ø§Ø±ÛŒÙˆ Ø¶Ø¹ÛŒÙØŒØ¨Ø§Ø²ÛŒ Ø§Ú©ØªÙˆ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Text (comment) Spam or ham \n",
       "0   ÛŒÙ‡ Ù…Ø´Øª Ù„Ø§Øª Ùˆ Ù„ÙˆØª Ø¬Ù…Ø¹ Ú©Ø±Ø¯ÛŒØ¯ ØªÙˆ Ø§ÛŒÙ† Ø§ØªØ§Ù‚ ÙØ±Ø§Ø± Ùˆ ...         Spam\n",
       "1   Ø³Ù†Ø§Ø±ÛŒÙˆ Ø§ØµÙ„Ø§ Ø®ÙˆØ¨ Ù†Ø¨ÙˆØ¯ Ùˆ Ø¨Ø±Ø§ÛŒ Ù…Ø§ Ù†ØµÙÙ‡ ØªÙ…ÙˆÙ… Ø´Ø¯ - ...         Spam\n",
       "2   Ø±ÙØªØ§Ø± Ù¾Ø±Ø³Ù†Ù„ Ù…Ù†Ø§Ø³Ø¨ Ù†Ø¨ÙˆØ¯\\n  Ø¹Ø¯Ù… Ø§Ú¯Ø§Ù‡ÛŒ Ø±Ø³Ø§Ù†ÛŒ Ø¯Ù‚ÛŒÙ‚...         Spam\n",
       "3   ğŸ˜¡ğŸ˜¡ğŸ˜¡Ù‡Ø´Ø¯Ø§Ø± Ø§ÛŒÙ† ÛŒÚ© Ú©Ù„Ø§Ù‡ Ø¨Ø±Ø¯Ø§Ø±ÛŒ Ø¹Ù„Ù†ÛŒ Ø§Ø³ØªğŸ˜¡ğŸ˜¡ğŸ˜¡\\nØ®ÙˆÙ†Ù‡ ...         Spam\n",
       "4   Ø§ØªØ§Ù‚ ÙØ±Ø§Ø± Ø®ÙˆØ¨ÛŒ Ø¨ÙˆØ¯ Ø§Ù…Ø§ Ù†Ù‡ Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú©Ø§Ù…Ù†Øª Ù‡Ø§ Ù‚...         Spam\n",
       "5   Ø³Ù„Ø§Ù…\\nÙ…Ø¬ÙˆØ² Ø¨Ø±Ø§ÛŒ Ø§ØªØ§Ù‚ ÙØ±Ø§Ø± Ù†Ø¯Ø§Ø´ØªÙ†Ø¯\\nØ¨Ø±Ø®ÙˆØ±Ø¯ Ø®ÙˆØ¨ ...         Spam\n",
       "6   Ø¯Ø§Ø³ØªØ§Ù† Ø¨Ø§Ø²ÛŒ Ø³Ø± Ùˆ ØªÙ‡ Ù…Ø´Ø®ØµÛŒ Ù†Ø¯Ø§Ø´ØªØŒØ¨ÛŒØ´ØªØ± ÙØ¶Ø§ Ø³Ø§Ø²ÛŒ...         Spam\n",
       "7   Ø¯Ø± Ø®ØµÙˆØµ Ø¨Ø§Ø²ÛŒ Ù‡Ø§ÛŒ Ø§ØªØ§Ù‚ ÙØ±Ø§Ø± Ø¨Ø§ Ø§Ø­ØªØ±Ø§Ù… Ø¨Ù‡ Ú©Ø³Ø§Ù†ÛŒ ...         Spam\n",
       "8   Ù†Ù‡ Ù…Ø¬ÙˆØ²\\nÙ†Ù‡ Ú©ÙˆÚ†Ù‡ Ù…Ø·Ù…Ø¦Ù† \\nÙ†Ù‡ Ù…Ø­Ù„Ù‡ Ø¯Ø±Ø³Øª\\nÙ†Ù‡ Ø¬Ø§ÛŒ ...         Spam\n",
       "9   Ø§ÙØªØ¶Ø§Ø­ Ø®ÙˆØ§Ù‡Ø´Ù† Ø¨Ù‡ ÙƒØ§Ù…Ù†Øª Ù‡Ø§ ØªÙˆØ¬Ù‡ Ù†ÙƒÙ†ÙŠØ¯\\nÙ†Ù…ÙŠØ¯ÙˆÙ†Ù… ...         Spam\n",
       "10  Ø³Ù„Ø§Ù… Ø§ØªØ§Ù‚ ÙˆØ§Ù‚Ø¹Ø§ ÙØ¶Ø§ÛŒ Ø¨Ø²Ø±Ú¯ÛŒ Ø¯Ø§Ø´Øª Ùˆ Ø·Ø¨Ù‚Ø§Øª Ø²ÛŒØ§Ø¯ Ø§...         Spam\n",
       "11                                       Ø¨ÛŒ Ù†Ø§Ù…ÙˆØ³Ø§Ø§Ø§Ø§         Spam\n",
       "12  Ø¨Ù‡ Ù†Ø¸Ø± Ù…Ù† ÙˆØ§Ù‚Ø¹Ø§ Ø§Ø² Ù‡ÛŒÚ† Ù†Ø¸Ø± Ø®ÙˆØ¨ Ù†Ø¨ÙˆØ¯ØŒ Ú†ÙˆÙ† Ù‚Ø¨Ù„ Ø§...         Spam\n",
       "13  Ø§ØµÙ„Ø§ Ø¬Ø§Ù„Ø¨ Ù†Ø¨ÙˆØ¯ Ù‚ÙÙ„ Ù‡Ø§ Ù‡Ù…Ù‡ Ø®Ø±Ø§Ø¨  Ù…Ø§ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ú©...         Spam\n",
       "14  Ø³Ù„Ø§Ù… Ù…Ø§ Ø¯ÛŒØ´Ø¨ Ø³Ø§Ø¹Øª Û±Û² Ø³Ø§Ù†Ø³ ÙˆÛŒÚ˜Ù‡ Ø¨Ø§Ø²ÛŒÙˆ Ø±Ø²Ø±Ùˆ Ú©Ø±Ø¯Ù‡...         Spam\n",
       "15  Ø§ÙØªØ¶Ø§Ø­\\nØ¯Ú©ÙˆØ± Ùˆ ÙØ¶Ø§ Ø³Ø§Ø²ÛŒ Ø®Ø¯Ø§ÛŒÛŒ Ø®ÙˆØ¨ Ø¨ÙˆØ¯ \\nÙˆÙ„ÛŒ Ø§Ú©...         Spam\n",
       "16  Ù…Ø§ Ø§ÙˆÙ„Ø´ Ø§Ù†ØµØ±Ø§Ù Ø¯Ø§Ø¯ÛŒÙ… ÙˆÙ„ÛŒ Ø±ÙØªØ§Ø± Ù¾Ø±Ø³Ù†Ù„ Ø¨Ø³ÛŒØ§Ø± Ø¨Ø¯ ...         Spam\n",
       "17  Ø¨Ø³ÛŒØ§Ø± Ø¶Ø¹ÛŒÙ Ø¨ÙˆØ¯.Ø¯Ú©ÙˆØ± Ø¶Ø¹ÛŒÙØŒØ³Ù†Ø§Ø±ÛŒÙˆ Ø¶Ø¹ÛŒÙØŒØ¨Ø§Ø²ÛŒ Ø§Ú©ØªÙˆ...         Spam"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '/Users/niloufar/Desktop/DeepLearning/tf_specialization/comment/'\n",
    "data1 = 'spam_or_not1.xlsx'\n",
    "data2 = 'spam_or_not2.xlsx'\n",
    "data3 = 'spam_or_not3.xlsx'\n",
    "\n",
    "df1 = pd.read_excel(data_path + data1)\n",
    "df2 = pd.read_excel(data_path + data2)\n",
    "df3 = pd.read_excel(data_path + data3)\n",
    "\n",
    "df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "df = df.drop(['ID', df.columns[3]], axis=1)\n",
    "df.head(18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62379bb",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b045576",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels'] = df.apply(lambda x: x[1].lower(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e4d00b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=df.columns[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1819956f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "ham     5520\n",
       "spam     715\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.columns[1]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5c9526e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11040"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grouped_by = df.groupby('labels')\n",
    "df_balanced = df_grouped_by.apply(lambda x: x.sample(df_grouped_by.size().max(), replace=True).reset_index(drop=True))\n",
    "len(df_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d44b2669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "ham     5520\n",
       "spam    5520\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced[df_balanced.columns[1]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d4b8119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Text (comment)'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced.columns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5394b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = {'class':['c1','c2','c1','c1','c2','c1','c1','c2','c3','c3'],\n",
    "#      'val': [1,2,1,1,2,1,1,2,3,3]\n",
    "#     }\n",
    "\n",
    "# df = pd.DataFrame(d)\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "375546d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = df.groupby('class')\n",
    "# b = g.apply(lambda x: x.sample(g.size().max(), replace=True).reset_index(drop=True))\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7783ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(sentence):\n",
    "    stopwords = ['Ú©Ù‡', 'Ø¯Ø±', 'Ø§Ø²', 'Ø¨Ù‡', 'Ùˆ', 'Ø±Ø§', 'Ø§ÛŒÙ†', 'Ø¢Ù†', 'Ø¨Ø¹Ø¯', 'Ù‡Ù…Ù‡', 'Ø¯ÙˆØ¨Ø§Ø±Ù‡', 'ÛŒÚ©', 'ÛŒÙ‡', 'Ù…Ù†', 'ØªÙˆ', 'Ø§Ùˆ', 'Ù…Ø§', 'Ø´Ù…Ø§', 'Ù‚Ø¨Ù„', 'Ø¢Ù†Ù‡Ø§', 'Ø²ÛŒØ±Ø§', 'Ø²ÛŒØ±', 'Ø§Ù…Ø§', 'Ø¨ÛŒÙ†', 'Ø¯Ùˆ', 'Ø¨Ø§', 'Ø§ÙˆÙ†Ø¬Ø§',\n",
    "                'Ø¨Ø±Ø§ÛŒ', 'Ø­ØªÙ…Ø§','Ø­Ø§Ù„ÛŒ', 'Ú†Ø±Ø§', 'Ú†ÛŒ', 'Ø§Ø²Ø·Ø±ÛŒÙ‚', 'Ø±Ùˆ', ',', 'ÙˆØ§Ù‚Ø¹Ø§','Ù‡Ø§', 'ØªÙˆ', 'Ø§ÙˆÙ†', 'ØªØ±ÛŒÙ†', 'ØªÙˆÛŒ', 'Ú†Ù‡', 'Ù…Ø§Ø±Ùˆ', 'Ø³Ø±', 'Ø§ÙˆÙ†Ø¬Ø§', 'Ø®ÙˆØ¯', 'Ù‡Ø§Ø±Ùˆ', 'Ø¢Ù‚Ø§', 'Ù‡Ù…ØªÙˆÙ†', 'Ù‡Ø§Ù…', 'Ø¯ÙˆØªØ§', 'Ø¯ÙˆØ¨Ø§Ø±Ù‡',\n",
    "                'Ø§Ú¯Ù‡', 'ÙˆÙ„ÛŒ', 'Ø±ÙˆØ´', 'Ø§ÛŒÙ†Ùˆ', 'Ù‡Ù†ÙˆØ²', 'Ø¯Ù‡', 'Ø³Ù‡', 'Ú†Ù‡Ø§Ø±', 'Ù¾Ù†Ø¬', 'Ø´Ø´', 'Ù‡ÙØª', 'Ù‡Ø´Øª', 'Ù†Ù‡', 'Ø¯Ù‡', 'Ù†Ø§','Ø§ÛŒÛŒ']\n",
    "    words = sentence.split()\n",
    "    results_words = [word for word in words if word not in stopwords]\n",
    "    sentence = ' '.join(results_words)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1b0b89f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ù¾Ø¯Ø± Ø³Ú¯ Ø¯Ø±Ø®Øª Ø¨Ø³Øª.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(\"Ù¾Ø¯Ø± Ø³Ú¯ Ø±Ø§ Ø¨Ù‡ Ø¯Ø±Ø®Øª Ø¨Ø³Øª.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6965fc20",
   "metadata": {},
   "source": [
    "### Reading the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b70cf449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_from_file(df):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for i,j in zip(df[df.columns[0]], df[df.columns[1]]):\n",
    "        sentences.append(remove_stopwords(i))\n",
    "        labels.append(j)\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1dc41920",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, labels = parse_data_from_file(df_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e082d1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11040 sentences in the dataset.\n",
      "\n",
      "First sentence has 30 words (after removing stopwords).\n",
      "\n",
      "There are 11040 labels in the dataset.\n",
      "\n",
      "The first 5 labels are ['ham', 'ham', 'ham', 'ham', 'ham']\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(sentences)} sentences in the dataset.\\n\")\n",
    "print(f\"First sentence has {len(sentences[0].split())} words (after removing stopwords).\\n\")\n",
    "print(f\"There are {len(labels)} labels in the dataset.\\n\")\n",
    "print(f\"The first 5 labels are {labels[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f0d073d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 10000 #10894\n",
    "EMBEDDING_DIM = 64\n",
    "MAXLEN = 120 #681\n",
    "PADDING = 'post'\n",
    "OOV_TOKEN = '<OOV>'\n",
    "TRAINING_SPLIT = .8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3642c075",
   "metadata": {},
   "source": [
    "### Training - Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8e9019e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(sentences, labels, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d2ae9d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_val_split(sentences, labels, training_split):\n",
    "#     train_size = int(len(sentences)*training_split)\n",
    "#     train_sentences = sentences[:train_size]\n",
    "#     train_labels = labels[:train_size]\n",
    "    \n",
    "#     validation_sentences = sentences[train_size:]\n",
    "#     validation_labels = labels[train_size:]\n",
    "#     return train_sentences, validation_sentences, train_labels, validation_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c93e29bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8832 sentences for training.\n",
      "\n",
      "There are 8832 labels for training.\n",
      "\n",
      "There are 2208 sentences for validation.\n",
      "\n",
      "There are 2208 labels for validation.\n"
     ]
    }
   ],
   "source": [
    "# train_sentences, val_sentences, train_labels, val_labels = train_val_split(sentences, labels, TRAINING_SPLIT)\n",
    "\n",
    "print(f\"There are {len(train_sentences)} sentences for training.\\n\")\n",
    "print(f\"There are {len(train_labels)} labels for training.\\n\")\n",
    "print(f\"There are {len(val_sentences)} sentences for validation.\\n\")\n",
    "print(f\"There are {len(val_labels)} labels for validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fcd8b5",
   "metadata": {},
   "source": [
    "### Using the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2871dcd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4423, 1097)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.count('spam'), val_labels.count('spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "73f46b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tokenizer(sentences, num_words, oov_token):\n",
    "    tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4caa9d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary contains 10786 words\n",
      "\n",
      "<OOV> token included in vocabulary\n"
     ]
    }
   ],
   "source": [
    "tokenizer = fit_tokenizer(train_sentences, NUM_WORDS, OOV_TOKEN)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(f\"Vocabulary contains {len(tokenizer.word_index)} words\\n\")\n",
    "print(\"<OOV> token included in vocabulary\" if \"<OOV>\" in tokenizer.word_index else \"<OOV> token NOT included in vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c46690b",
   "metadata": {},
   "source": [
    "### Save tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ee5dfffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open('tokenizer.pickle', 'wb') as handle:\n",
    "#     pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ec185d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_and_pad(tokenizer, sentences, padding, maxlen):\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    padded_sequences = pad_sequences(sequences, padding=padding, maxlen=maxlen)\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3ff32ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded training sequences have shape: (8832, 120)\n",
      "\n",
      "Padded validation sequences have shape: (2208, 120)\n"
     ]
    }
   ],
   "source": [
    "train_padded_seq = seq_and_pad(tokenizer,train_sentences, PADDING, MAXLEN)\n",
    "val_padded_seq = seq_and_pad(tokenizer, val_sentences, PADDING, MAXLEN)\n",
    "\n",
    "print(f\"Padded training sequences have shape: {train_padded_seq.shape}\\n\")\n",
    "print(f\"Padded validation sequences have shape: {val_padded_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f73a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_labels(all_labels, split_labels):\n",
    "#     label_tokenizer = Tokenizer()\n",
    "#     label_tokenizer.fit_on_texts(all_labels)\n",
    "#     label_seq = label_tokenizer.texts_to_sequences(split_labels)\n",
    "#     label_seq_np = np.array(label_seq) - 1\n",
    "    \n",
    "#     return label_seq_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90ee8cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('spam', 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_seq = np.unique(train_labels, return_inverse=True)[1] # spam==1\n",
    "train_labels[0], train_label_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba146e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('spam', 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_label_seq =np.unique(val_labels, return_inverse=True)[1]  # spam==1\n",
    "val_labels[0], val_label_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c98c36f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 labels of the training set should look like this:\n",
      "[1 1 0 1 1]\n",
      "\n",
      "First 5 labels of the validation set should look like this:\n",
      "[1 0 1 0 1]\n",
      "\n",
      "Tokenized labels of the training set have shape: (8832,)\n",
      "\n",
      "Tokenized labels of the validation set have shape: (2208,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train_label_seq = tokenize_labels(labels, train_labels)\n",
    "# val_label_seq = tokenize_labels(labels, val_labels)\n",
    "\n",
    "print(f\"First 5 labels of the training set should look like this:\\n{train_label_seq[:5]}\\n\")\n",
    "print(f\"First 5 labels of the validation set should look like this:\\n{val_label_seq[:5]}\\n\")\n",
    "print(f\"Tokenized labels of the training set have shape: {train_label_seq.shape}\\n\")\n",
    "print(f\"Tokenized labels of the validation set have shape: {val_label_seq.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e263abf1",
   "metadata": {},
   "source": [
    "### Selecting the model for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e747cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_words, embedding_dim, maxlen):\n",
    "    \n",
    "    tf.random.set_seed(123)\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(num_words, embedding_dim, input_length=MAXLEN),\n",
    "#         tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(6, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb85724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(NUM_WORDS, EMBEDDING_DIM, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd63c804",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_padded_seq, train_label_seq, epochs=30, validation_data=(val_padded_seq, val_label_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82480cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(val_padded_seq[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5152ec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_label_seq[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfba567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_graphs(history, metric):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history[f'val_{metric}'])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([metric, f'val_{metric}'])\n",
    "    plt.show()\n",
    "    \n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f94f2e4",
   "metadata": {},
   "source": [
    "### Save the entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5458e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p ../../saved_model\n",
    "model.save('../../saved_model/my_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ecabfacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 11:13:10.948468: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.32601893]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model('../../saved_model/my_model')\n",
    "new_model.predict(train_padded_seq[1:2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf38",
   "language": "python",
   "name": "tf38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
