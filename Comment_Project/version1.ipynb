{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7ad7f01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e168f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text (comment)</th>\n",
       "      <th>Spam or ham</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ÛŒÙ‡ Ù…Ø´Øª Ù„Ø§Øª Ùˆ Ù„ÙˆØª Ø¬Ù…Ø¹ Ú©Ø±Ø¯ÛŒØ¯ ØªÙˆ Ø§ÛŒÙ† Ø§ØªØ§Ù‚ ÙØ±Ø§Ø± Ùˆ ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ø³Ù†Ø§Ø±ÛŒÙˆ Ø§ØµÙ„Ø§ Ø®ÙˆØ¨ Ù†Ø¨ÙˆØ¯ Ùˆ Ø¨Ø±Ø§ÛŒ Ù…Ø§ Ù†ØµÙÙ‡ ØªÙ…ÙˆÙ… Ø´Ø¯ - ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ø±ÙØªØ§Ø± Ù¾Ø±Ø³Ù†Ù„ Ù…Ù†Ø§Ø³Ø¨ Ù†Ø¨ÙˆØ¯\\n  Ø¹Ø¯Ù… Ø§Ú¯Ø§Ù‡ÛŒ Ø±Ø³Ø§Ù†ÛŒ Ø¯Ù‚ÛŒÙ‚...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ğŸ˜¡ğŸ˜¡ğŸ˜¡Ù‡Ø´Ø¯Ø§Ø± Ø§ÛŒÙ† ÛŒÚ© Ú©Ù„Ø§Ù‡ Ø¨Ø±Ø¯Ø§Ø±ÛŒ Ø¹Ù„Ù†ÛŒ Ø§Ø³ØªğŸ˜¡ğŸ˜¡ğŸ˜¡\\nØ®ÙˆÙ†Ù‡ ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ø§ØªØ§Ù‚ ÙØ±Ø§Ø± Ø®ÙˆØ¨ÛŒ Ø¨ÙˆØ¯ Ø§Ù…Ø§ Ù†Ù‡ Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú©Ø§Ù…Ù†Øª Ù‡Ø§ Ù‚...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ø³Ù„Ø§Ù…\\nÙ…Ø¬ÙˆØ² Ø¨Ø±Ø§ÛŒ Ø§ØªØ§Ù‚ ÙØ±Ø§Ø± Ù†Ø¯Ø§Ø´ØªÙ†Ø¯\\nØ¨Ø±Ø®ÙˆØ±Ø¯ Ø®ÙˆØ¨ ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ø¯Ø§Ø³ØªØ§Ù† Ø¨Ø§Ø²ÛŒ Ø³Ø± Ùˆ ØªÙ‡ Ù…Ø´Ø®ØµÛŒ Ù†Ø¯Ø§Ø´ØªØŒØ¨ÛŒØ´ØªØ± ÙØ¶Ø§ Ø³Ø§Ø²ÛŒ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ø¯Ø± Ø®ØµÙˆØµ Ø¨Ø§Ø²ÛŒ Ù‡Ø§ÛŒ Ø§ØªØ§Ù‚ ÙØ±Ø§Ø± Ø¨Ø§ Ø§Ø­ØªØ±Ø§Ù… Ø¨Ù‡ Ú©Ø³Ø§Ù†ÛŒ ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ù†Ù‡ Ù…Ø¬ÙˆØ²\\nÙ†Ù‡ Ú©ÙˆÚ†Ù‡ Ù…Ø·Ù…Ø¦Ù† \\nÙ†Ù‡ Ù…Ø­Ù„Ù‡ Ø¯Ø±Ø³Øª\\nÙ†Ù‡ Ø¬Ø§ÛŒ ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ø§ÙØªØ¶Ø§Ø­ Ø®ÙˆØ§Ù‡Ø´Ù† Ø¨Ù‡ ÙƒØ§Ù…Ù†Øª Ù‡Ø§ ØªÙˆØ¬Ù‡ Ù†ÙƒÙ†ÙŠØ¯\\nÙ†Ù…ÙŠØ¯ÙˆÙ†Ù… ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Ø³Ù„Ø§Ù… Ø§ØªØ§Ù‚ ÙˆØ§Ù‚Ø¹Ø§ ÙØ¶Ø§ÛŒ Ø¨Ø²Ø±Ú¯ÛŒ Ø¯Ø§Ø´Øª Ùˆ Ø·Ø¨Ù‚Ø§Øª Ø²ÛŒØ§Ø¯ Ø§...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ø¨ÛŒ Ù†Ø§Ù…ÙˆØ³Ø§Ø§Ø§Ø§</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Ø¨Ù‡ Ù†Ø¸Ø± Ù…Ù† ÙˆØ§Ù‚Ø¹Ø§ Ø§Ø² Ù‡ÛŒÚ† Ù†Ø¸Ø± Ø®ÙˆØ¨ Ù†Ø¨ÙˆØ¯ØŒ Ú†ÙˆÙ† Ù‚Ø¨Ù„ Ø§...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Ø§ØµÙ„Ø§ Ø¬Ø§Ù„Ø¨ Ù†Ø¨ÙˆØ¯ Ù‚ÙÙ„ Ù‡Ø§ Ù‡Ù…Ù‡ Ø®Ø±Ø§Ø¨  Ù…Ø§ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ú©...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ø³Ù„Ø§Ù… Ù…Ø§ Ø¯ÛŒØ´Ø¨ Ø³Ø§Ø¹Øª Û±Û² Ø³Ø§Ù†Ø³ ÙˆÛŒÚ˜Ù‡ Ø¨Ø§Ø²ÛŒÙˆ Ø±Ø²Ø±Ùˆ Ú©Ø±Ø¯Ù‡...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Ø§ÙØªØ¶Ø§Ø­\\nØ¯Ú©ÙˆØ± Ùˆ ÙØ¶Ø§ Ø³Ø§Ø²ÛŒ Ø®Ø¯Ø§ÛŒÛŒ Ø®ÙˆØ¨ Ø¨ÙˆØ¯ \\nÙˆÙ„ÛŒ Ø§Ú©...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Ù…Ø§ Ø§ÙˆÙ„Ø´ Ø§Ù†ØµØ±Ø§Ù Ø¯Ø§Ø¯ÛŒÙ… ÙˆÙ„ÛŒ Ø±ÙØªØ§Ø± Ù¾Ø±Ø³Ù†Ù„ Ø¨Ø³ÛŒØ§Ø± Ø¨Ø¯ ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Ø¨Ø³ÛŒØ§Ø± Ø¶Ø¹ÛŒÙ Ø¨ÙˆØ¯.Ø¯Ú©ÙˆØ± Ø¶Ø¹ÛŒÙØŒØ³Ù†Ø§Ø±ÛŒÙˆ Ø¶Ø¹ÛŒÙØŒØ¨Ø§Ø²ÛŒ Ø§Ú©ØªÙˆ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Text (comment) Spam or ham \n",
       "0   ÛŒÙ‡ Ù…Ø´Øª Ù„Ø§Øª Ùˆ Ù„ÙˆØª Ø¬Ù…Ø¹ Ú©Ø±Ø¯ÛŒØ¯ ØªÙˆ Ø§ÛŒÙ† Ø§ØªØ§Ù‚ ÙØ±Ø§Ø± Ùˆ ...         Spam\n",
       "1   Ø³Ù†Ø§Ø±ÛŒÙˆ Ø§ØµÙ„Ø§ Ø®ÙˆØ¨ Ù†Ø¨ÙˆØ¯ Ùˆ Ø¨Ø±Ø§ÛŒ Ù…Ø§ Ù†ØµÙÙ‡ ØªÙ…ÙˆÙ… Ø´Ø¯ - ...         Spam\n",
       "2   Ø±ÙØªØ§Ø± Ù¾Ø±Ø³Ù†Ù„ Ù…Ù†Ø§Ø³Ø¨ Ù†Ø¨ÙˆØ¯\\n  Ø¹Ø¯Ù… Ø§Ú¯Ø§Ù‡ÛŒ Ø±Ø³Ø§Ù†ÛŒ Ø¯Ù‚ÛŒÙ‚...         Spam\n",
       "3   ğŸ˜¡ğŸ˜¡ğŸ˜¡Ù‡Ø´Ø¯Ø§Ø± Ø§ÛŒÙ† ÛŒÚ© Ú©Ù„Ø§Ù‡ Ø¨Ø±Ø¯Ø§Ø±ÛŒ Ø¹Ù„Ù†ÛŒ Ø§Ø³ØªğŸ˜¡ğŸ˜¡ğŸ˜¡\\nØ®ÙˆÙ†Ù‡ ...         Spam\n",
       "4   Ø§ØªØ§Ù‚ ÙØ±Ø§Ø± Ø®ÙˆØ¨ÛŒ Ø¨ÙˆØ¯ Ø§Ù…Ø§ Ù†Ù‡ Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú©Ø§Ù…Ù†Øª Ù‡Ø§ Ù‚...         Spam\n",
       "5   Ø³Ù„Ø§Ù…\\nÙ…Ø¬ÙˆØ² Ø¨Ø±Ø§ÛŒ Ø§ØªØ§Ù‚ ÙØ±Ø§Ø± Ù†Ø¯Ø§Ø´ØªÙ†Ø¯\\nØ¨Ø±Ø®ÙˆØ±Ø¯ Ø®ÙˆØ¨ ...         Spam\n",
       "6   Ø¯Ø§Ø³ØªØ§Ù† Ø¨Ø§Ø²ÛŒ Ø³Ø± Ùˆ ØªÙ‡ Ù…Ø´Ø®ØµÛŒ Ù†Ø¯Ø§Ø´ØªØŒØ¨ÛŒØ´ØªØ± ÙØ¶Ø§ Ø³Ø§Ø²ÛŒ...         Spam\n",
       "7   Ø¯Ø± Ø®ØµÙˆØµ Ø¨Ø§Ø²ÛŒ Ù‡Ø§ÛŒ Ø§ØªØ§Ù‚ ÙØ±Ø§Ø± Ø¨Ø§ Ø§Ø­ØªØ±Ø§Ù… Ø¨Ù‡ Ú©Ø³Ø§Ù†ÛŒ ...         Spam\n",
       "8   Ù†Ù‡ Ù…Ø¬ÙˆØ²\\nÙ†Ù‡ Ú©ÙˆÚ†Ù‡ Ù…Ø·Ù…Ø¦Ù† \\nÙ†Ù‡ Ù…Ø­Ù„Ù‡ Ø¯Ø±Ø³Øª\\nÙ†Ù‡ Ø¬Ø§ÛŒ ...         Spam\n",
       "9   Ø§ÙØªØ¶Ø§Ø­ Ø®ÙˆØ§Ù‡Ø´Ù† Ø¨Ù‡ ÙƒØ§Ù…Ù†Øª Ù‡Ø§ ØªÙˆØ¬Ù‡ Ù†ÙƒÙ†ÙŠØ¯\\nÙ†Ù…ÙŠØ¯ÙˆÙ†Ù… ...         Spam\n",
       "10  Ø³Ù„Ø§Ù… Ø§ØªØ§Ù‚ ÙˆØ§Ù‚Ø¹Ø§ ÙØ¶Ø§ÛŒ Ø¨Ø²Ø±Ú¯ÛŒ Ø¯Ø§Ø´Øª Ùˆ Ø·Ø¨Ù‚Ø§Øª Ø²ÛŒØ§Ø¯ Ø§...         Spam\n",
       "11                                       Ø¨ÛŒ Ù†Ø§Ù…ÙˆØ³Ø§Ø§Ø§Ø§         Spam\n",
       "12  Ø¨Ù‡ Ù†Ø¸Ø± Ù…Ù† ÙˆØ§Ù‚Ø¹Ø§ Ø§Ø² Ù‡ÛŒÚ† Ù†Ø¸Ø± Ø®ÙˆØ¨ Ù†Ø¨ÙˆØ¯ØŒ Ú†ÙˆÙ† Ù‚Ø¨Ù„ Ø§...         Spam\n",
       "13  Ø§ØµÙ„Ø§ Ø¬Ø§Ù„Ø¨ Ù†Ø¨ÙˆØ¯ Ù‚ÙÙ„ Ù‡Ø§ Ù‡Ù…Ù‡ Ø®Ø±Ø§Ø¨  Ù…Ø§ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ú©...         Spam\n",
       "14  Ø³Ù„Ø§Ù… Ù…Ø§ Ø¯ÛŒØ´Ø¨ Ø³Ø§Ø¹Øª Û±Û² Ø³Ø§Ù†Ø³ ÙˆÛŒÚ˜Ù‡ Ø¨Ø§Ø²ÛŒÙˆ Ø±Ø²Ø±Ùˆ Ú©Ø±Ø¯Ù‡...         Spam\n",
       "15  Ø§ÙØªØ¶Ø§Ø­\\nØ¯Ú©ÙˆØ± Ùˆ ÙØ¶Ø§ Ø³Ø§Ø²ÛŒ Ø®Ø¯Ø§ÛŒÛŒ Ø®ÙˆØ¨ Ø¨ÙˆØ¯ \\nÙˆÙ„ÛŒ Ø§Ú©...         Spam\n",
       "16  Ù…Ø§ Ø§ÙˆÙ„Ø´ Ø§Ù†ØµØ±Ø§Ù Ø¯Ø§Ø¯ÛŒÙ… ÙˆÙ„ÛŒ Ø±ÙØªØ§Ø± Ù¾Ø±Ø³Ù†Ù„ Ø¨Ø³ÛŒØ§Ø± Ø¨Ø¯ ...         Spam\n",
       "17  Ø¨Ø³ÛŒØ§Ø± Ø¶Ø¹ÛŒÙ Ø¨ÙˆØ¯.Ø¯Ú©ÙˆØ± Ø¶Ø¹ÛŒÙØŒØ³Ù†Ø§Ø±ÛŒÙˆ Ø¶Ø¹ÛŒÙØŒØ¨Ø§Ø²ÛŒ Ø§Ú©ØªÙˆ...         Spam"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '/Users/niloufar/Desktop/DeepLearning/tf_specialization/comment/'\n",
    "data1 = 'spam_or_not1.xlsx'\n",
    "data2 = 'spam_or_not2.xlsx'\n",
    "data3 = 'spam_or_not3.xlsx'\n",
    "\n",
    "df1 = pd.read_excel(data_path + data1)\n",
    "df2 = pd.read_excel(data_path + data2)\n",
    "df3 = pd.read_excel(data_path + data3)\n",
    "\n",
    "df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "df = df.drop(['ID', df.columns[3]], axis=1)\n",
    "df.head(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7783ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(sentence):\n",
    "    stopwords = ['Ú©Ù‡', 'Ø¯Ø±', 'Ø§Ø²', 'Ø¨Ù‡', 'Ùˆ', 'Ø±Ø§', 'Ø§ÛŒÙ†', 'Ø¢Ù†', 'Ø¨Ø¹Ø¯', 'Ù‡Ù…Ù‡', 'Ø¯ÙˆØ¨Ø§Ø±Ù‡', 'ÛŒÚ©', 'ÛŒÙ‡', 'Ù…Ù†', 'ØªÙˆ', 'Ø§Ùˆ', 'Ù…Ø§', 'Ø´Ù…Ø§', 'Ù‚Ø¨Ù„', 'Ø¢Ù†Ù‡Ø§', 'Ø²ÛŒØ±Ø§', 'Ø²ÛŒØ±', 'Ø§Ù…Ø§', 'Ø¨ÛŒÙ†', 'Ø¯Ùˆ', 'Ø¨Ø§', 'Ø§ÙˆÙ†Ø¬Ø§',\n",
    "                'Ø¨Ø±Ø§ÛŒ', 'Ø­ØªÙ…Ø§','Ø­Ø§Ù„ÛŒ', 'Ú†Ø±Ø§', 'Ú†ÛŒ', 'Ø§Ø²Ø·Ø±ÛŒÙ‚', 'Ø±Ùˆ', ',', 'ÙˆØ§Ù‚Ø¹Ø§','Ù‡Ø§', 'ØªÙˆ', 'Ø§ÙˆÙ†', 'ØªØ±ÛŒÙ†', 'ØªÙˆÛŒ', 'Ú†Ù‡', 'Ù…Ø§Ø±Ùˆ', 'Ø³Ø±', 'Ø§ÙˆÙ†Ø¬Ø§', 'Ø®ÙˆØ¯', 'Ù‡Ø§Ø±Ùˆ', 'Ø¢Ù‚Ø§', 'Ù‡Ù…ØªÙˆÙ†', 'Ù‡Ø§Ù…', 'Ø¯ÙˆØªØ§', 'Ø¯ÙˆØ¨Ø§Ø±Ù‡',\n",
    "                'Ø§Ú¯Ù‡', 'ÙˆÙ„ÛŒ', 'Ø±ÙˆØ´', 'Ø§ÛŒÙ†Ùˆ', 'Ù‡Ù†ÙˆØ²', 'Ø¯Ù‡', 'Ø³Ù‡', 'Ú†Ù‡Ø§Ø±', 'Ù¾Ù†Ø¬', 'Ø´Ø´', 'Ù‡ÙØª', 'Ù‡Ø´Øª', 'Ù†Ù‡', 'Ø¯Ù‡', 'Ù†Ø§','Ø§ÛŒÛŒ']\n",
    "    words = sentence.split()\n",
    "    results_words = [word for word in words if word not in stopwords]\n",
    "    sentence = ' '.join(results_words)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b0b89f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ù¾Ø¯Ø± Ø³Ú¯ Ø¯Ø±Ø®Øª Ø¨Ø³Øª.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(\"Ù¾Ø¯Ø± Ø³Ú¯ Ø±Ø§ Ø¨Ù‡ Ø¯Ø±Ø®Øª Ø¨Ø³Øª.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6965fc20",
   "metadata": {},
   "source": [
    "### Reading the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b70cf449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_from_file(file):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for i,j in zip(df[df.columns[0]], df[df.columns[1]]):\n",
    "        sentences.append(remove_stopwords(i))\n",
    "        labels.append(j)\n",
    "    return sentences, labels\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1dc41920",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, labels = parse_data_from_file(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e082d1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6235 sentences in the dataset.\n",
      "\n",
      "First sentence has 83 words (after removing stopwords).\n",
      "\n",
      "There are 6235 labels in the dataset.\n",
      "\n",
      "The first 5 labels are ['Spam', 'Spam', 'Spam', 'Spam', 'Spam']\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(sentences)} sentences in the dataset.\\n\")\n",
    "print(f\"First sentence has {len(sentences[0].split())} words (after removing stopwords).\\n\")\n",
    "print(f\"There are {len(labels)} labels in the dataset.\\n\")\n",
    "print(f\"The first 5 labels are {labels[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fcd8b5",
   "metadata": {},
   "source": [
    "### Using the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7559aa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 13861\n",
    "EMBEDDING_DIM = 16\n",
    "MAXLEN = 608\n",
    "PADDING = 'post'\n",
    "OOV_TOKEN = '<OOV>'\n",
    "TRAINING_SPLIT = .8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2871dcd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5519"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.count('ham')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73f46b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tokenizer(sentences):\n",
    "    tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4caa9d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary contains 13861 words\n",
      "\n",
      "<OOV> token included in vocabulary\n"
     ]
    }
   ],
   "source": [
    "tokenizer = fit_tokenizer(sentences)\n",
    "print(f\"Vocabulary contains {len(tokenizer.word_index)} words\\n\")\n",
    "print(\"<OOV> token included in vocabulary\" if \"<OOV>\" in tokenizer.word_index else \"<OOV> token NOT included in vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec185d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padded_sequences(tokenizer, sentences):\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    padded_sequences = pad_sequences(sequences, padding='post')\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ff32ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First padded sequence looks like this: \n",
      "\n",
      "[3148 5654 5655  554  794    7   12   91 1463  161  236    3  693  324\n",
      "    3  108 1292 3954 5656  738   45  657 3955 1690 1464 5657  132 1111\n",
      " 3956 3957  202 5658  458  402 5659 5660  764   21 5661 5662    6 1377\n",
      "   63 1690 1464  554  238  390 5663 1293  980   96 3149 2039 1842  307\n",
      " 5664   10   77   28  295 5665  284 3958 5666  158    7   12 5667 1378\n",
      "  493 1112  981 5668  473  412 5669   25 5670 2301 3959    7   12 2040\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0]\n",
      "\n",
      "Numpy array of all sequences has shape: (6235, 681)\n",
      "\n",
      "This means there are 6235 sequences in total and each one has a size of 681\n"
     ]
    }
   ],
   "source": [
    "padded_sequences = get_padded_sequences(tokenizer, sentences)\n",
    "print(f\"First padded sequence looks like this: \\n\\n{padded_sequences[0]}\\n\")\n",
    "print(f\"Numpy array of all sequences has shape: {padded_sequences.shape}\\n\")\n",
    "print(f\"This means there are {padded_sequences.shape[0]} sequences in total and each one has a size of {padded_sequences.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8f73a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_labels(labels):\n",
    "    label_tokenizer = Tokenizer()\n",
    "    label_tokenizer.fit_on_texts(labels)\n",
    "    label_word_index = label_tokenizer.word_index\n",
    "    label_sequences = label_tokenizer.texts_to_sequences(labels)\n",
    "    return label_sequences, label_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c98c36f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary of labels looks like this {'ham': 1, 'spam': 2}\n",
      "\n",
      "First ten sequences [[2], [2], [2], [2], [2], [2], [2], [2], [2], [2]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_sequences, label_word_index = tokenize_labels(labels)\n",
    "print(f\"Vocabulary of labels looks like this {label_word_index}\\n\")\n",
    "print(f\"First ten sequences {label_sequences[:10]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34c78218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_seq_np = np.array(label_sequences) - 1\n",
    "label_seq_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e263abf1",
   "metadata": {},
   "source": [
    "### Training - Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b9b79060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(sentences, label_seq_np, training_split):\n",
    "    train_size = int(len(sentences)*training_split)\n",
    "    train_sentences = sentences[:train_size]\n",
    "    train_labels = labels[:train_size]\n",
    "    \n",
    "    validation_sentences = sentences[train_size:]\n",
    "    validation_labels = labels[train_size:]\n",
    "    return train_sentences, validation_sentences, train_labels, validation_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4a606be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4988 sentences for training.\n",
      "\n",
      "There are 4988 labels for training.\n",
      "\n",
      "There are 1247 sentences for validation.\n",
      "\n",
      "There are 1247 labels for validation.\n"
     ]
    }
   ],
   "source": [
    "train_sentences, val_sentences, train_labels, val_labels = train_val_split(sentences, labels, TRAINING_SPLIT)\n",
    "\n",
    "print(f\"There are {len(train_sentences)} sentences for training.\\n\")\n",
    "print(f\"There are {len(train_labels)} labels for training.\\n\")\n",
    "print(f\"There are {len(val_sentences)} sentences for validation.\\n\")\n",
    "print(f\"There are {len(val_labels)} labels for validation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c5e747cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_words, embedding_dim, maxlen):\n",
    "    \n",
    "    tf.random.set_seed(123)\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(num_words, embedding_dim, input_length=maxlen),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(6, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cb85724d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 11:36:28.917687: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-05-03 11:36:28.918288: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 608, 16)           221776    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9728)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 58374     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 280,157\n",
      "Trainable params: 280,157\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(NUM_WORDS, EMBEDDING_DIM, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd63c804",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_padded_seq, train_label_seq, epochs=30, validation_data=(val_padded_seq, val_label_seq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82480cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5152ec55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfba567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10edd8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88a07fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d93c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de78331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27636208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eb74df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf38",
   "language": "python",
   "name": "tf38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
