{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7ad7f01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e168f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text (comment)</th>\n",
       "      <th>Spam or ham</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>یه مشت لات و لوت جمع کردید تو این اتاق فرار و ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>سناریو اصلا خوب نبود و برای ما نصفه تموم شد - ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>رفتار پرسنل مناسب نبود\\n  عدم اگاهی رسانی دقیق...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>😡😡😡هشدار این یک کلاه برداری علنی است😡😡😡\\nخونه ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>اتاق فرار خوبی بود اما نه به اندازه کامنت ها ق...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>سلام\\nمجوز برای اتاق فرار نداشتند\\nبرخورد خوب ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>داستان بازی سر و ته مشخصی نداشت،بیشتر فضا سازی...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>در خصوص بازی های اتاق فرار با احترام به کسانی ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>نه مجوز\\nنه کوچه مطمئن \\nنه محله درست\\nنه جای ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>افتضاح خواهشن به كامنت ها توجه نكنيد\\nنميدونم ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>سلام اتاق واقعا فضای بزرگی داشت و طبقات زیاد ا...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>بی ناموساااا</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>به نظر من واقعا از هیچ نظر خوب نبود، چون قبل ا...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>اصلا جالب نبود قفل ها همه خراب  ما برای پیدا ک...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>سلام ما دیشب ساعت ۱۲ سانس ویژه بازیو رزرو کرده...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>افتضاح\\nدکور و فضا سازی خدایی خوب بود \\nولی اک...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ما اولش انصراف دادیم ولی رفتار پرسنل بسیار بد ...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>بسیار ضعیف بود.دکور ضعیف،سناریو ضعیف،بازی اکتو...</td>\n",
       "      <td>Spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Text (comment) Spam or ham \n",
       "0   یه مشت لات و لوت جمع کردید تو این اتاق فرار و ...         Spam\n",
       "1   سناریو اصلا خوب نبود و برای ما نصفه تموم شد - ...         Spam\n",
       "2   رفتار پرسنل مناسب نبود\\n  عدم اگاهی رسانی دقیق...         Spam\n",
       "3   😡😡😡هشدار این یک کلاه برداری علنی است😡😡😡\\nخونه ...         Spam\n",
       "4   اتاق فرار خوبی بود اما نه به اندازه کامنت ها ق...         Spam\n",
       "5   سلام\\nمجوز برای اتاق فرار نداشتند\\nبرخورد خوب ...         Spam\n",
       "6   داستان بازی سر و ته مشخصی نداشت،بیشتر فضا سازی...         Spam\n",
       "7   در خصوص بازی های اتاق فرار با احترام به کسانی ...         Spam\n",
       "8   نه مجوز\\nنه کوچه مطمئن \\nنه محله درست\\nنه جای ...         Spam\n",
       "9   افتضاح خواهشن به كامنت ها توجه نكنيد\\nنميدونم ...         Spam\n",
       "10  سلام اتاق واقعا فضای بزرگی داشت و طبقات زیاد ا...         Spam\n",
       "11                                       بی ناموساااا         Spam\n",
       "12  به نظر من واقعا از هیچ نظر خوب نبود، چون قبل ا...         Spam\n",
       "13  اصلا جالب نبود قفل ها همه خراب  ما برای پیدا ک...         Spam\n",
       "14  سلام ما دیشب ساعت ۱۲ سانس ویژه بازیو رزرو کرده...         Spam\n",
       "15  افتضاح\\nدکور و فضا سازی خدایی خوب بود \\nولی اک...         Spam\n",
       "16  ما اولش انصراف دادیم ولی رفتار پرسنل بسیار بد ...         Spam\n",
       "17  بسیار ضعیف بود.دکور ضعیف،سناریو ضعیف،بازی اکتو...         Spam"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '/Users/niloufar/Desktop/DeepLearning/tf_specialization/comment/'\n",
    "data1 = 'spam_or_not1.xlsx'\n",
    "data2 = 'spam_or_not2.xlsx'\n",
    "data3 = 'spam_or_not3.xlsx'\n",
    "\n",
    "df1 = pd.read_excel(data_path + data1)\n",
    "df2 = pd.read_excel(data_path + data2)\n",
    "df3 = pd.read_excel(data_path + data3)\n",
    "\n",
    "df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "df = df.drop(['ID', df.columns[3]], axis=1)\n",
    "df.head(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7783ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(sentence):\n",
    "    stopwords = ['که', 'در', 'از', 'به', 'و', 'را', 'این', 'آن', 'بعد', 'همه', 'دوباره', 'یک', 'یه', 'من', 'تو', 'او', 'ما', 'شما', 'قبل', 'آنها', 'زیرا', 'زیر', 'اما', 'بین', 'دو', 'با', 'اونجا',\n",
    "                'برای', 'حتما','حالی', 'چرا', 'چی', 'ازطریق', 'رو', ',', 'واقعا','ها', 'تو', 'اون', 'ترین', 'توی', 'چه', 'مارو', 'سر', 'اونجا', 'خود', 'هارو', 'آقا', 'همتون', 'هام', 'دوتا', 'دوباره',\n",
    "                'اگه', 'ولی', 'روش', 'اینو', 'هنوز', 'ده', 'سه', 'چهار', 'پنج', 'شش', 'هفت', 'هشت', 'نه', 'ده', 'نا','ایی']\n",
    "    words = sentence.split()\n",
    "    results_words = [word for word in words if word not in stopwords]\n",
    "    sentence = ' '.join(results_words)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b0b89f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'پدر سگ درخت بست.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(\"پدر سگ را به درخت بست.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6965fc20",
   "metadata": {},
   "source": [
    "### Reading the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b70cf449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_from_file(file):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for i,j in zip(df[df.columns[0]], df[df.columns[1]]):\n",
    "        sentences.append(remove_stopwords(i))\n",
    "        labels.append(j)\n",
    "    return sentences, labels\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1dc41920",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, labels = parse_data_from_file(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e082d1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6235 sentences in the dataset.\n",
      "\n",
      "First sentence has 83 words (after removing stopwords).\n",
      "\n",
      "There are 6235 labels in the dataset.\n",
      "\n",
      "The first 5 labels are ['Spam', 'Spam', 'Spam', 'Spam', 'Spam']\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(sentences)} sentences in the dataset.\\n\")\n",
    "print(f\"First sentence has {len(sentences[0].split())} words (after removing stopwords).\\n\")\n",
    "print(f\"There are {len(labels)} labels in the dataset.\\n\")\n",
    "print(f\"The first 5 labels are {labels[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fcd8b5",
   "metadata": {},
   "source": [
    "### Using the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7559aa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 13861\n",
    "EMBEDDING_DIM = 16\n",
    "MAXLEN = 608\n",
    "PADDING = 'post'\n",
    "OOV_TOKEN = '<OOV>'\n",
    "TRAINING_SPLIT = .8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2871dcd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5519"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.count('ham')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73f46b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tokenizer(sentences):\n",
    "    tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4caa9d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary contains 13861 words\n",
      "\n",
      "<OOV> token included in vocabulary\n"
     ]
    }
   ],
   "source": [
    "tokenizer = fit_tokenizer(sentences)\n",
    "print(f\"Vocabulary contains {len(tokenizer.word_index)} words\\n\")\n",
    "print(\"<OOV> token included in vocabulary\" if \"<OOV>\" in tokenizer.word_index else \"<OOV> token NOT included in vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec185d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padded_sequences(tokenizer, sentences):\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    padded_sequences = pad_sequences(sequences, padding='post')\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ff32ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First padded sequence looks like this: \n",
      "\n",
      "[3148 5654 5655  554  794    7   12   91 1463  161  236    3  693  324\n",
      "    3  108 1292 3954 5656  738   45  657 3955 1690 1464 5657  132 1111\n",
      " 3956 3957  202 5658  458  402 5659 5660  764   21 5661 5662    6 1377\n",
      "   63 1690 1464  554  238  390 5663 1293  980   96 3149 2039 1842  307\n",
      " 5664   10   77   28  295 5665  284 3958 5666  158    7   12 5667 1378\n",
      "  493 1112  981 5668  473  412 5669   25 5670 2301 3959    7   12 2040\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0]\n",
      "\n",
      "Numpy array of all sequences has shape: (6235, 681)\n",
      "\n",
      "This means there are 6235 sequences in total and each one has a size of 681\n"
     ]
    }
   ],
   "source": [
    "padded_sequences = get_padded_sequences(tokenizer, sentences)\n",
    "print(f\"First padded sequence looks like this: \\n\\n{padded_sequences[0]}\\n\")\n",
    "print(f\"Numpy array of all sequences has shape: {padded_sequences.shape}\\n\")\n",
    "print(f\"This means there are {padded_sequences.shape[0]} sequences in total and each one has a size of {padded_sequences.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8f73a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_labels(labels):\n",
    "    label_tokenizer = Tokenizer()\n",
    "    label_tokenizer.fit_on_texts(labels)\n",
    "    label_word_index = label_tokenizer.word_index\n",
    "    label_sequences = label_tokenizer.texts_to_sequences(labels)\n",
    "    return label_sequences, label_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c98c36f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary of labels looks like this {'ham': 1, 'spam': 2}\n",
      "\n",
      "First ten sequences [[2], [2], [2], [2], [2], [2], [2], [2], [2], [2]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_sequences, label_word_index = tokenize_labels(labels)\n",
    "print(f\"Vocabulary of labels looks like this {label_word_index}\\n\")\n",
    "print(f\"First ten sequences {label_sequences[:10]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34c78218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_seq_np = np.array(label_sequences) - 1\n",
    "label_seq_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e263abf1",
   "metadata": {},
   "source": [
    "### Training - Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b9b79060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(sentences, label_seq_np, training_split):\n",
    "    train_size = int(len(sentences)*training_split)\n",
    "    train_sentences = sentences[:train_size]\n",
    "    train_labels = labels[:train_size]\n",
    "    \n",
    "    validation_sentences = sentences[train_size:]\n",
    "    validation_labels = labels[train_size:]\n",
    "    return train_sentences, validation_sentences, train_labels, validation_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4a606be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4988 sentences for training.\n",
      "\n",
      "There are 4988 labels for training.\n",
      "\n",
      "There are 1247 sentences for validation.\n",
      "\n",
      "There are 1247 labels for validation.\n"
     ]
    }
   ],
   "source": [
    "train_sentences, val_sentences, train_labels, val_labels = train_val_split(sentences, labels, TRAINING_SPLIT)\n",
    "\n",
    "print(f\"There are {len(train_sentences)} sentences for training.\\n\")\n",
    "print(f\"There are {len(train_labels)} labels for training.\\n\")\n",
    "print(f\"There are {len(val_sentences)} sentences for validation.\\n\")\n",
    "print(f\"There are {len(val_labels)} labels for validation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c5e747cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_words, embedding_dim, maxlen):\n",
    "    \n",
    "    tf.random.set_seed(123)\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(num_words, embedding_dim, input_length=maxlen),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(6, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cb85724d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 11:36:28.917687: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-05-03 11:36:28.918288: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 608, 16)           221776    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9728)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 58374     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 280,157\n",
      "Trainable params: 280,157\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(NUM_WORDS, EMBEDDING_DIM, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd63c804",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_padded_seq, train_label_seq, epochs=30, validation_data=(val_padded_seq, val_label_seq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82480cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5152ec55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfba567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10edd8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88a07fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d93c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de78331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27636208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eb74df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf38",
   "language": "python",
   "name": "tf38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
